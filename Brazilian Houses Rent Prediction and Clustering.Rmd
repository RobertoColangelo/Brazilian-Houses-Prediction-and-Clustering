---
title: "Brazilian Houses Rent Prediction and Clustering"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
For our course of Data Analysis for Business we were asked to perform a data analysis and regression/classification together with a secondary task on a dataset of our choice among those provided by the professor. For such purpose, we decided to choose the Brazilian Houses Rent dataset since it seemed to be one of the most interesting and challenging one. The tasks that were asked to be performed on the dataset were essentially two:

1) Creating many different predictive regression models for predicting the rent amount of a property

2) Clustering the instances according to the characteristic of interest of the dataset

We initially started the job by performing some data exploration and cleaning of the variables and successively we focused more on the modeling part. The results that came out were quite surprising and showed that the best model was a non parametric one such as the Random Forest Regressor.

## Importing Libraries
We started importing all the needed libraries for performing all the tasks to be solved (Data cleaning, Pre-processing, Modeling and Clustering).

```{r Library,echo=TRUE, include=TRUE,results='hide',warning=FALSE,message=FALSE}
library(reshape2) #Required for data transformation
library(memisc) #Required for finding missing values
library(glmnet) #Required for regularization models
library(gbm) #Required for boosting model
library(caret) #Required for train-test partition and pre-processing
library(xgboost) #Required for Extreme Gradient Boosting model
library(randomForest) #Required for Random Forest model
library(pls) #Required for Principal component regression
library(tidyverse) #Required for dplyr and ggplot2
library(dplyr) #Required for data manipulation
library(ggplot2) #Required for data visualization 
library(GGally) #For pairwise plots
library(factoextra) #Required for PCA visualization
library(MASS) 
library(gridExtra)
library(cluster)
```

## Importing the file and Data Exploration
After importing the libraries needed,we imported the dataset from the csv file and performed some data exploration.

The dataset is made of 10692 instances and 12 features, and ,among these, only 4 are character variables (categorical). This ,as will be seen later on, led us to transform them through OnehotEncoding and dummy variables in order to use them in our predictive models. Successively we also computed some statistics to see important information about the numeric variables.

```{r Importing and exploration, echo=T}
#Importing csv file 
df <- read.csv(file = '/Users/robertocolangelo/Desktop/Progetto Brazilian Houses/Data Analysis final project/BrazHousesRent.csv')

# Dimensions of the dataset (# of Instances and columns)
dim(df) #10692 instances and 12 attributes

#Glimpse of the typology of each variable.
str(df) #4 categorical variables to substitute, the rest are numerical
#Computing some  statistics
summary(df)
```
```{r dataframe, echo=T}
head(df)
```
## Data Cleaning

By taking a look at some variables,we noticed that there were some missing values and subsequently we decided to look for null values and remove them together with the duplicates. In order to remove the null values (that were only in the column floor) we had to substitute the '-' character with NaN by hand since R seemed to not recognize such character as Null value. This led to a reduction of the instances from 10692 to 7960 but was necessary to improve the future performances of our statistical models. Moreover,as specified in the guidelines, we also checked for unary columns (columns made by just one value of all the strings)but ,as displayed by the code, there were none.
```{r data cleaning, echo=T}
#Null values 
df[df=="-"]<-NA
colSums(is.na(df))
which(colSums(is.na(df))>0)
names(which(colSums(is.na(df))>0))
#Only floor has Null values 

df<-na.omit(df)
#Let's remove them

#Removing duplicates and creating a new dataframe (df) with no duplicates
df<-unique(df)
dim(df) #The number of instances has diminished from 10692 to 7960

#eliminating unary columns (unary means with only one value in a column) (not relevant for our modeling)
for(i in list(seq=1,12,1)){
  if(length(unique(df[,i]))<2)
    df[,i]<-NULL
}
dim(df) #Same number of columns meaning that there are not unary columns
```
## Data Visualization

After cleaning the data a bit (Not the variables yet), we plotted some relationships between the variables in order to understand the interdependencies among what will be our target variable and the other , and also among the independent variables themselves.

The first plot consisted in a scatterplot of Rent Amount against Taxes on property. However we decided to remove some outliers and then a high leverage point to better identify the patterns and relationship between these two variables.

```{r Plot1, echo=T}
#Relationship between rent and taxes on property
dfr<-df
ggplot(aes(y = property.tax..R.., x = rent.amount..R..  ), data = dfr) + 
  geom_point(alpha = 0.35, pch = 1) + 
  labs(y= 'Taxes ',
       x= 'Rent',
       title= 'Relationship of Rent Vs. Taxes')
#There is an outlier --> We'll remove it and replot the relationship between the two variable

which(dfr$property.tax..R..>20000) 
dfr<-dfr[-c(4719,5006),]
#dfr<-dfr[-c(1683,6105,6493),] #Removing instances number 1683,6104,6492
ggplot(aes(y = property.tax..R.., x = rent.amount..R..  ), data = dfr) + 
  geom_point(alpha = 0.35, pch = 1) + 
  labs(y= 'Taxes ',
       x= 'Rent',
       title= 'Relationship of Rent Vs. Taxes')
#Removing high leverage point 
which(df$rent.amount..R..>21000)
dfr<-dfr[-1956,]
ggplot(aes(y = property.tax..R.., x = rent.amount..R..  ), data = dfr) + 
  geom_point(alpha = 0.35, pch = 1) + 
  labs(y= 'Taxes ',
       x= 'Rent',
       title= 'Relationship of Rent Vs. Taxes')
```

As can be seen from the plot above most of the rents are concentrated in the range between 0 and 15000 while taxes from 0 to 6000. What appears interesting is the fact that taxes don't have a linear relationship with rents,this means that a higher rent doesn't necessarily consist in higher taxes to pay for the property.

Afterwards we decided to plot an histogram to understand whose City among the five in the dataset whose the one with the highest number of properties in the dataset:
```{r Plot2, echo=T}
#Barchart of number properties per city 
ggplot(df) + 
  geom_bar(aes( x = city,fill=city, ))+ 
  theme(axis.text.x = element_text(vjust=0.5, angle = 90))+
  labs(y= 'Number of Properties',
      x= 'Density',
      title= 'Number of properties per city')
```

Most of the Properties are located in Sao Paulo which, despite not being the capital, is the most populous and wealthiest state in Brazil.

Then we plotted a violin plot to have some insights on the distribution of the prices of rents for each city:
```{r Plot3, echo=T}
ggplot(df)+
  geom_violin(aes(x=city,y=rent.amount..R..,fill=city))+
  theme(axis.text.x = element_text(vjust=0.5, angle = 90))
```

As can be seen from the violin distribution plot of rents the place which costs the most is still Sao Paulo(The shape of the violin is longer and a bit more skewed toward the upper part),while it appears that the cheapest rents are in Campinas.

Subsequently we also took a look at the relationships between rent and furnished or not furnished properties and Property area vs number of  bathrooms
```{r Plot4, echo=T}
ggplot(df) +
  geom_boxplot( aes(x=furniture, y=rent.amount..R.., fill=furniture))+
  theme(axis.text.x = element_text(vjust=0.5, angle = 60))
```

On average places with no furniture cost less but there are some not furnished place that cost even more top furnished places

```{r Plot5, echo=T}
dfg<-df
which(df$area>10000)
dfg<-dfg[-c(2365,5786,8958),]
dfg$bathroom<-as_factor(dfg$bathroom)
ggplot(aes(y = area, x = bathroom,fill=bathroom), data = dfg) + 
  geom_boxplot() + 
  labs(y= 'area ',
       x= 'Number of bathrooms',
       title= 'House Space Vs. Number of Bathrooms')
which(dfg$area>1500)
dfg<-dfg[-c(1789,4476,6569,6906),]
dfg$bathroom<-as_factor(dfg$bathroom)
ggplot(aes(y = area, x = bathroom,fill=bathroom), data = dfg) + 
  geom_boxplot() + 
  labs(y= 'area ',
       x= 'Number of bathrooms',
       title= 'House Space Vs. Number of Bathrooms')
```

Obviously the larger the property area, the higher the number of bathrooms.

## Distribution of the Target Variable Rent Amount

Before starting building the statistical model we had to check whether the output variable Y followed a Gaussian Distribution or not.

This is particularly important in case we want to implement linear models since we know that one of the assumptions of linear models is that the target variable has to follow a Normal distribution
```{r Plot6, echo=T}
dfy<-df$rent.amount
distrib<-hist(dfy, breaks=20 , col="yellow", xlab="Rent",
              main="Distribution of Rent")
xfit<-seq(min(dfy),max(dfy),length=40)
yfit<-dnorm(xfit,mean=mean(dfy),sd=sd(dfy))
yfit <- yfit*diff(distrib$mids[1:2])*length(dfy)
lines(xfit, yfit, col="black ", lwd=2)
```

Since the rent doesn't seem to follow a Gaussian distribution we decided to transform it with a logarithm :

```{r Plot7, echo=T}
dfylog<-log(dfy)
distrib<-hist(dfylog, breaks=20 , col="yellow", xlab="Rent",
              main="Distribution of Rent")
xfit<-seq(min(dfylog),max(dfylog),length=40)
yfit<-dnorm(xfit,mean=mean(dfylog),sd=sd(dfylog))
yfit <- yfit*diff(distrib$mids[1:2])*length(dfylog)
lines(xfit, yfit, col="black ", lwd=2)
```

## Data Preparation and Pre-Processing

As stated before, four of our features are categorical and cannot be used in such form in many statistical models.

For such reason we had to convert them by handling each case separately in the following way:

- The variables Animal and Furniture could be handled by creating two binary variables taking the value of 1 (for Animal 'accepted' or 'furnished' property) and 0 (for Animal 'not accepted' or 'not furnished' property)

- The variable floor was transformed by turning the categorical variables into numeric ones (it was tough though reaching such conclusion since it could have badly impacted the model ,but in the end it didn't)

- The variable city which was the hardest to transform since it could not be converted as an ordinal categorical variable given the fact that there wasn't a hierarchical relationship among the values of the feature. For such reason we opted to convert the different cities by using k-1 dummy variable by using the DummyVars function of the library caret.

```{r Feature Engineering, echo=T}
#Converting animal allowance and furniture as binary variables
df$animal<-ifelse(df$animal=='acept',1,0)
df$furniture<-ifelse(df$furniture=='furnished',1,0)
#might consider converting Cities with one Hot encoding (there is no hierarchical order...) and removing/converting floor to numeric because too many variable would be created with onehotencoding
df$floor<-as.numeric(df$floor)
dummy <- dummyVars(" ~ .", data=df)
df <- data.frame(predict(dummy, newdata = df))
dforiginal<-df
```

## Correlation Matrices

After doing this we built a correlation matrix with the dummy variables for the cities and another one with no dummies to better understand the correlation between the variables and, in particular,our target variable rent amount.

```{r Plot8, echo=T,warning=F}
#Correlation matrix between all variables 
df[,]%>%
  cor()%>%
  melt()%>%
  ggplot(aes(Var1,Var2, fill=value))+
  scale_fill_gradient2(low = "#075AFF",
                       mid = "#FFFFCC",
                       high = "#FF0000")+
  geom_tile(color='black')+
  geom_text(aes(label=paste(round(value,2))),size=2, color='black')+
  theme(axis.text.x = element_text(vjust=0.5, angle = 65))+
  labs(title='Correlation between variables',
       x='',y='')
#Most important correlations with target variable rent amount are:
#rooms->Number of rooms
#fire.insurance--> monthly fire insurance cost, kind strange it is =1 
#parking.spaces --> private parking spaces included with the house

#Correlation without one hot encoded variables 
dfnocity<-df[,-c(1,2,3,4,5)]
head(dfnocity)
dfnocity[,]%>%
  cor()%>%
  melt()%>%
  ggplot(aes(Var1,Var2, fill=value))+
  scale_fill_gradient2(low = "#075AFF",
                       mid = "#FFFFCC",
                       high = "#FF0000")+
  geom_tile(color='black')+
  geom_text(aes(label=paste(round(value,2))),size=2, color='black')+
  theme(axis.text.x = element_text(vjust=0.5, angle = 65))+
  labs(title='Correlation between variables',
       x='',y='')
```

## Models Implementation

### Train-Test Split

Before proceeding to fit our predictive models we perform a train-test split in order to create our train set for training our models and our test set to evaluate them. Before doing this we set a seed to avoid inconsistencies when re-running our code. As target variable we'll use the column 14 (rent.amount) and as independent variables all the others.
```{r train-test }
#Setting seed 
set.seed(12)
df$rent.amount..R..<-log(df$rent.amount..R..) #Trasforming output variable with log for linear regression assumptions
training <- df$rent.amount..R.. %>%
  createDataPartition(p = 0.75, list = FALSE)
trainset  <- df[training, ]
xtrain<-trainset[,-14]
ytrain<-trainset[,14]
testset<- df[-training, ]
xtest<-testset[,-14]
ytest<-testset[,14]
```

### Scaling Variables
```{r scaling}
xtrain <- xtrain %>% scale()
xtest <- xtest %>% scale(center=attr(xtrain, "scaled:center"), 
                         scale=attr(xtrain, "scaled:scale"))
```

## Parametric Methods
### Baseline Models

We'll start by creating two linear regression that we'll use as baseline to compare the performance of the other statistical learning methods.  In order to compare the different models we'll use the Mean Squared Error (MSE) and for each one of them we'll store the performance in global variables that,at the end, will be inserted in a table for comparing them.

The first model will be a simple linear regression having as predictor the variable rooms:

```{r}
base <- lm(rent.amount..R.. ~ rooms, data = trainset)
summary(base) #Rsquared== 0.266--> low
```

Interpretation of the coefficients-> since we transformed the target with a log operation  the impact of a unit increase of X is of e^(beta1) on Y. In this case the impact of an additional room on the value of Y (rent amount ) is of e^(0.3821) which is 1.4623.
If we have one more room , Y increases by 1.46 

```{r}
predictions<-predict(base,testset[,-14])
#We'll use Mean squared error as metric
baselineMSE=mean((ytest-predictions)^2) #MSE 0.42
baselineMSE
```

MSE is equal to 0.42 for this model. Our objective will be to diminish the MSE as much as possible.

Second baseline model: Linear regression with the two most correlated variables with Y (rooms and fire.insurance)

```{r}
base2 <- lm(rent.amount..R..~fire.insurance..R..+ rooms, data = trainset)
summary(base2)#full model has adjusted Rsquared of 0.833--> improved a lot, fire insurance is super powerful in the predictive model
predictions<-predict(base2,testset[,-14])
baseline2MSE=mean((ytest-predictions)^2)#MSE 0.09 -> way better.
baseline2MSE
```

It's surprising to see that adding the variable fire.insurance has a huge impact on prediction performance by reducing it a lot (-0.33 on MSE)

### Full Linear Regression (with all the predictors)

```{r Fulllr, warning=F}
fulllr<- lm(rent.amount..R..~., data = trainset)
summary(fulllr)#full model has adjusted Rsquared of 0.88-->really high on trainset 
predictions<-predict(fulllr,testset[,-14])
multivariatelrMSE=mean((predictions-ytest)^2)  #MSE 0.07
multivariatelrMSE
```
By using all the variables the model improves even more and MSE goes down to 0.07

### Stepwise Selection Models

We'll try with all the three typologies of stepwise selection models by using the Akaike Information Criterion (AIC) to select a subset of variables:

```{r Stepwise Mixed}
#Forward and back
AICforback<- step(fulllr, direction = "both", trace = F)
summary(AICforback)
AICforbackMSE=mean(AICforback$residuals^2) #MSE 0.068 -> improved (removes the following variables: animal, property tax,area and hoa)
AICforbackMSE
```

```{r Stepwise Forward}
#Forward
modelstepaicforward<- step( fulllr, direction = "forward", trace = F)
summary(modelstepaicforward)
AICforMSE=mean(modelstepaicforward$residuals^2)
AICforMSE#MSE 0.068  
```
```{r Stepwise Backward}
#Back
modelstepaicback<- step( fulllr, direction = "backward", trace = F)
summary(modelstepaicback)
AICbackMSE=mean(modelstepaicback$residuals^2)
AICbackMSE #MSE 0.068
```
They all yield pretty much the same results and reduce even more MSE to 0.068

By looking at the summary, animal and property.tax don't seem to be statistically significant so we'll remove them from our model and try to fit linear model by also adding an interaction term made by the variables room and area
```{r}
trainnew<-trainset[,c(-11,-15)]
testnew<-testset[,c(-11,-15)]
#Models with interaction terms of a subset of variables
interaction<-lm(rent.amount..R..~ rooms +I(rooms*area) +furniture+hoa..R..+floor+parking.spaces+bathroom+cityBelo.Horizonte+cityCampinas+cityPorto.Alegre+cityRio.de.Janeiro+citySão.Paulo,data=trainnew)
summary(interaction) #Adjusted Rsquared 0.55 interaction between area and rooms impact negatively on the prediction so let's not consider interactions 
lrinteractionMSE=mean(interaction$residuals^2) #MSE 0.259
lrinteractionMSE
```
This interaction term has a negative predictive impact on the model so it is better to not consider it.

### Principal Component Analysis Regression

Just for studying purposes we'll also try with a principal component analysis regression (which  is used for different situations such as high dimensional datasets and leads to a reduction in variability of the data and will surely yield worse performances with respect to the previous methods).

```{r}
#Visualizing number of components needed 
pca <- princomp(trainset[,-14], 
                cor = TRUE,
                score = TRUE)
pca$loadings
summary(pca)
```
Variance explained by the different components can be seen in the cumulative proportion row.

```{r}
fviz_eig(pca) #By using elbow method it is clear that 3 components are more than enough
```

From the scree plot above and according to the elbow method it is clear that the optimal number of components is 3, so we'll use only the first 3 principal components.

```{r}
#Principal component regression 
pcreg <- pcr(rent.amount..R..~., data = trainset, scale = TRUE, validation = "CV")
pcrpred <- predict(pcreg, testset[,-14], ncomp = 3)
pcrMSE=mean((pcrpred - testset[,14])^2) #MSE 0.18-> obviously we lose a bit of variability from creating new variables from the original ones but now we only have 3 dimensions (variables)
pcrMSE
```

As forecasted before the performances are worse than all the other models but better than the baseline 1.

### Regularization Models

For what concerns the regularization models we identified the best parameters through a grid search and implemented the following models:
- A 10 folds Ridge Regression
- A 20 folds Ridge Regression
- A Lasso Regression
- An Elastic Net
Shrinkage methods shrink the value of the coefficients of the irrelevant predictors towards 0 (ridge regression) or exactly to 0 (lasso regression)

### Ridge 10 folds
```{r}
folds <- 10
control <- trainControl(method="cv", number=folds)

#Ridge 10 folds
nlambdas <- 100
lambdas <- seq(0.001, 2, length.out = nlambdas)
trainmatrix<-as.matrix(trainset)
testmatrix<-as.matrix(testset)
x<-trainmatrix[,-14]
y<-trainmatrix[,14]
ridgereg10 <- cv.glmnet(x, y,nfolds=folds, alpha=0)
ridgereg10<- train(x, y, method = "glmnet", trControl = control,
                   tuneGrid = expand.grid(alpha = 0, 
                                          lambda = lambdas))
predictions<-predict(ridgereg10,as.matrix(testmatrix[,-14]))
ridgereg10MSE=mean((predictions - testmatrix[,14])^2 )#MSE 0.076 pretty good but not as good as linear regression
ridgereg10MSE
```

Yields an MSE of 0.076 not much different from the Multivariate Linear Regression
```{r}
ridgereg10$bestTune
```

Just an insight on the parameter selected through the grid search

```{r}
folds<-20
nlambdas <- 100
lambdas <- seq(0.001, 2, length.out = nlambdas)
trainmatrix<-as.matrix(trainset)
testmatrix<-as.matrix(testset)
x<-trainmatrix[,-14]
y<-trainmatrix[,14]
ridgereg20<- cv.glmnet(x, y,nfolds=folds, alpha=0)
ridgereg20<- train(x, y, method = "glmnet", trControl = control,
                   tuneGrid = expand.grid(alpha = 0, 
                                          lambda = lambdas))
predictions<-predict(ridgereg20,as.matrix(testmatrix[,-14]))
ridgereg20MSE=mean((predictions - testmatrix[,14])^2 )#MSE 0.076 increasing the number of folds doesn't impact
ridgereg20MSE
```

Even in this case the MSE is 0.076 meaning that increasing the number of folds doesn't really improve the predictive performances of the model

```{r}
ridgereg20$bestTune
```


### Lasso Regression 10 folds

```{r,warning=F}
#Lasso 10 folds
fold=10
lambdas <- 10^seq(2, -3, by = -.1)
nalphas <- 20
alphas <- seq(0, 1, length.out=nalphas)
ctrl <- trainControl(method="cv", number=folds)
lassoreg <- train(x, y, method = "glmnet", trControl = control,
                  tuneGrid = expand.grid(alpha = alphas, 
                                         lambda = lambdas))
lassoreg$bestTune$alpha#Best parameters 
lassoreg$bestTune$lambda
predictions<-predict(lassoreg,as.matrix(testmatrix [,-14]))
lassoregMSE=mean((predictions - testmatrix[,14])^2) #MSE 0.072 BETTER THAN RIDGE--> shrinking some coefficients directly to 0 gives bettere results
lassoregMSE
```

The first two values above are,respectively, the values selected by the grid search for alpha and lambda ,while the last one is the MSE.

Compared to the two ridge regression it seems like shrinking some coefficients directly to 0 improves a lot the predictive performances of the model.

### Elastic Net

Elastic Net is another regularization methods that is a combination between lasso and ridge. The rate at which it is influenced by one or the other is determined by the parameter alpha. The closer alpha to 0 the more it resembles a lasso in shrinking the coefficients, the closer alpha to 1 the more it resembles a ridge. We'll try out 100 different values of alpha in the grid search.

```{r,warning=F}
#Elastic net -> Takes a while to run
lambda.grid<-10^seq(2,-2,length=100)
alpha.grid<-seq(0,1,length=100) #Trying out 100 different values of alpha between 0 and 1 to understand which is the best alpha
grid<-expand.grid(alpha=alpha.grid,lambda=lambda.grid)
Elnet<-train(x, y, method = "glmnet", trControl = ctrl,tuneGrid = grid)
Elnet$bestTune #Best parameters
predictions<-predict(Elnet,as.matrix(testmatrix [,-14]))
ElasticnetMSE=mean((predictions - testmatrix[,14])^2)
ElasticnetMSE#MSE 0.072
```

## Non-Parametric Methods

Finally we implemented three typologies of non-parametric method:
- A Random Forest Regressor model
- A Gradient Boosting Regressor model
- An XGBoost model

### Random Forest Regressor

In order to select the number of trees, we tried out many combinations but the one that gave us the best results was by using 600 trees
```{r}
#Random Forest 
Rfreg <- randomForest(
  formula = rent.amount..R..~ .,
  data= trainset,
  importance=FALSE,
  ntree=600 #Setting number of trees to train Random Forest model.The higher the number the more the time to execute the code
)
predictions<-predict(Rfreg,testset[-14])
RfregMSE=mean((predictions - testset[,14])^2) #MSE 0.008 almost perfect on the testset
RfregMSE
```

Random forest gave us the best result so far with an MSE of 0.0087 (almost perfect)
### Gradient Boosting Regressor

```{r}
# Gradient Boosting
boostreg= gbm(rent.amount..R.. ~.,
              data = trainset,
              distribution='gaussian',
              cv.folds = 10,
              shrinkage = .1,
              n.minobsinnode = 10,
              n.trees = 500 )
summary(boostreg)
predictions<-predict(boostreg,testset[,-14])
boostregMSE=mean((predictions - testset[,14])^2) #0.01
boostregMSE
```
Random forest performed better but Gradient Boosting performed pretty good with an MSE of 0.01
### XGBoost Regressor

```{r}
#Xgboost 
xgboost = xgboost(data = trainmatrix[,-14], label = trainmatrix[,14], nrounds = 150, 
                  objective = "reg:squarederror", eval_metric = "error")
predxgboost<-predict(xgboost,testmatrix[,-14])
XGBMSE=mean((predxgboost - testset[,14])^2) #0.009, a little bit better than gradient boosting but still not as good as random forest
XGBMSE
```
Even XGBoost performed great yielding results really close to Random Forest and close to a 0 error.
## Results Comparison

In then end we decided to create a list made up of all the MSEs of all the models and a list made by the names of each model and insert it in a dataframe to compare them. Subsequently we ordered them and investigated on the best one. What appears from the table below is that the models that performed better were the non-parametric methods,followed by the stepwise ones,  thus meaning that the interdependencies among the data were surely not linear. 
```{r}
models<-c('baseline','baseline 2','Multivariate LR',"Step_forwardAIC", "Step_backwardAIC", "MixedAIC",'Interaction',"PCR", "Lasso",'Elastic Net',"Ridge 10","Ridge 20", "RandomForest", "Boosting",'XGBoost')
r=c(baselineMSE,baseline2MSE,multivariatelrMSE,AICforMSE, AICbackMSE, AICforbackMSE,lrinteractionMSE, pcrMSE, lassoregMSE,ElasticnetMSE, ridgereg10MSE,ridgereg20MSE, RfregMSE, boostregMSE,XGBMSE)
Performancesummary <- data.frame(Model = models, MSE = r)
Performancesummary[order(Performancesummary$MSE),][1:5,]#Prints 5 most performing models
```

Here there is a plot displaying the differences in performance. It's important to remember that the lower the MSE, the better the performance of the model.

```{r}
Performanceplot<- ggplot(data = Performancesummary) + 
  geom_point(mapping = aes(x = Model, y=MSE,colour='Red'),show.legend=FALSE) +
  labs(x = 'Models', y = 'MSE')
Performanceplot+theme(axis.text.x=element_text(angle=90))
```

## Clustering

As requested in the task 2, we executed two different clustering algorithm, Kmeans and hierarchical. to group the instances according to their main characteristics. We tried out with both all the variables and just two variables (rooms, rent amount) and plotted the results in 4 pairplots for each number of clusters going from 2 to 5.

## Kmeans Clustering

We started clustering by using all the predictors but as can be seen below the variation of the WCSS were quite disappointing and the clusters were not really distinguishable and interpretable
```{r}
# Kmeans Clustering 
set.seed(13)
k2 <- kmeans(dforiginal,centers=2,nstart=10)
k2WSS<-k2$tot.withinss
k3 <- kmeans(dforiginal, centers = 3, nstart = 10)
k3WSS<-k3$tot.withinss
k4 <- kmeans(dforiginal, centers = 4, nstart = 10)
k4WSS<-k4$tot.withinss
k5 <- kmeans(dforiginal, centers = 5, nstart = 10)
k5WSS<-k5$tot.withinss
WSS1<-c(k2WSS,k3WSS,k4WSS,k5WSS)
Nclust<-c(2,3,4,5)
Clustercomparisonwss<-data.frame(WCSS=WSS1,K=Nclust)
Clustercomparisonwss[order(Clustercomparisonwss$WCSS),]
#Not really a great variation of Within Cluster Sum of Squares
```
Pair Plots of the different results for each number of cluster selected:
```{r}
p1 <- fviz_cluster(k2, geom = "point", data = dforiginal) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = dforiginal) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = dforiginal) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = dforiginal) + ggtitle("k = 5")
grid.arrange(p1, p2, p3, p4, nrow = 2)
#Really messy clusters that interpolate with one another
```

As can be seen the cluster interpolate with one another and are not interpretable

Afterwards we decided to use only two variables and the results yielded were a lot more satisfying:

```{r}
dfcluster<-dforiginal[,c(7,14)]
km2 <- kmeans(dfcluster,centers=2,nstart=25)
km2WSS<-km2$tot.withinss
km3<- kmeans(dfcluster, centers = 3, nstart = 25)
km3WSS<-km3$tot.withinss
km4<- kmeans(dfcluster, centers = 4, nstart = 25)
km4WSS<-km4$tot.withinss
km5<- kmeans(dfcluster, centers = 5, nstart = 25)
km5WSS<-km5$tot.withinss
WSS2<-c(km2WSS,km3WSS,km4WSS,km5WSS)
Nclust<-c(2,3,4,5)
Clustercomparisonwss<-data.frame(WCSS=WSS2,K=Nclust)
Clustercomparisonwss[order(Clustercomparisonwss$WCSS),]
#Obviously the more clusters,the less Within cluster sum of squares
```

Here the reduction in the WCSS is much more significant as the number of cluster increases.

```{r}
pm1 <- fviz_cluster(km2, geom = "point", data = dfcluster) + ggtitle("k = 2")
pm2 <- fviz_cluster(km3, geom = "point",  data = dfcluster) + ggtitle("k = 3")
pm3 <- fviz_cluster(km4, geom = "point",  data = dfcluster) + ggtitle("k = 4")
pm4 <- fviz_cluster(km5, geom = "point",  data = dfcluster) + ggtitle("k = 5")
grid.arrange(pm1, pm2, pm3, pm4, nrow = 2)
```
What appears from the plots above is that the clusters are mostly affected by the prices of the rent which divides the different observations in ranges.

### Hierarchical Clustering

Then we repeated the exact same steps but by using Hierarchical Clustering and obtained similar results when using all the variables and when using only rooms and rent amount.

For what concerns the distance we chose the euclidean one and as method the complete linkage and we chose to display the variation of WSS as done before with the Kmeans clustering in order to identify the optimal number of clusters to use in our algorithm.
```{r,warning=FALSE}
d1 <- dist(dforiginal, method = "euclidean")
hc1 <- hclust(d1, method = "complete" ) #Complete linkage
fviz_nbclust(dforiginal, FUN = hcut, method = "wss") #Takes a while to run and shows the best number of clusters is 2 according to elbow method
```

From the variation of WSS and by using the elbow method,it appears that the best number of cluster is 2. 

Successively we plotted the clusters and still obtained quite messy clusters that interpolate each other:

```{r,warning=FALSE}
sub_grp1 <- cutree(hc1, k = 2)
plot(sub_grp1, cex = 0.6, hang = -1)
fviz_cluster(list(data = dforiginal, cluster = sub_grp1),geom = "point")
```
We executed the algorithm again by trying to cluster only according to the variables rooms and rent amount and by using as before as distance the euclidean distance and as method of clustering the complete linkage. By plotting the variations of WSS it appeared that the optimal number of clusters is 3

```{r,warning=FALSE}
#Hierarchical clustering with only 2 variables
d2 <- dist(dfcluster, method = "euclidean")
hc2 <- hclust(d2, method = "complete" ) #Complete linkage
fviz_nbclust(dfcluster, FUN = hcut, method = "wss") #Takes a while to run and shows the best number of clusters is 3 according to elbow method
```

Finally we plotted again the graphs of the 3 clusters
```{r,warning=F}
sub_grp2 <- cutree(hc2, k = 3)
plot(sub_grp2, cex = 0.6, hang = -1)
fviz_cluster(list(data = dfcluster, cluster = sub_grp2),geom = "point")
```

## Conclusions

As requested in task 1, after performing some data exploration, cleaning and visualization we built many different predictive models. Out of all the methods the ones that seem to perform better are non-parametric (RFregressor, Gradient Boosting and XGBoost) and all yield MSE results pretty close to 0 with the Random Forest Regressor outperforming the other two by just 0.001.

For what concerns the clustering instead , we did it with both all variables and just the two most relevant and it appears that doing it by using all of them doesn't give interpretable cluster or results. Using just two of them allows us to say that the instances are grouped by the algorithm according to the price ranges of the rent amount.

In the case of the Hierarchical clustering algorithm the optimal number of cluster identified is 3, while in the case of the kmeans clustering is 5. This shows how much using different clustering algorithms can lead to different results and outcomes. 



